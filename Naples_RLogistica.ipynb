{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1688,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as special\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1689,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) host_response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\thost_response_time\n",
    "•\thost_acceptance_rate\n",
    "•\thost_is_superhost\n",
    "•\tnumber_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sin especificar' 'within a few hours' 'within an hour'\n",
      " 'a few days or more' 'within a day']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_response_time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_response_time\n",
      "Rápido    7972\n",
      "Lento     3055\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_response_time a dicotomica\n",
    "Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n",
    "    \"within an hour\": \"Rápido\",\n",
    "    \"within a few hours\": \"Rápido\",\n",
    "    \"within a day\": \"Lento\",\n",
    "    \"a few days or more\": \"Lento\",\n",
    "    \"Sin especificar\": \"Lento\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_response_time\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f' 'Sin especificar' 't']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_is_superhost\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1693,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\1419231691.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\"f\": 0, \"t\": 1, \"Sin especificar\": 2})\n"
     ]
    }
   ],
   "source": [
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\"f\": 0, \"t\": 1, \"Sin especificar\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_is_superhost\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_acceptance_rate', 'host_is_superhost', 'number_of_reviews']]\n",
    "Var_Dep = Naples['host_response_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rápido', 'Lento', 'Rápido', ..., 'Rápido', 'Rápido', 'Lento'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 394  487]\n",
      " [ 177 2251]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.8221329437545654\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# rapido o lento - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Rápido\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.7993351465699607\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.9271004942339374\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Rápido\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) host_is_superhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\thost_is_superhost\n",
    "•\thost_response_rate\n",
    "•\thost_acceptance_rate\n",
    "•\thost_total_listings_count\n",
    "•\t// host_response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "f                  7814\n",
      "t                  2825\n",
      "Sin especificar     388\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sin especificar' 'within a few hours' 'within an hour'\n",
      " 'a few days or more' 'within a day']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_response_time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "f    8202\n",
      "t    2825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n",
    "    \"Sin especificar\": \"f\",\n",
    "    # \"Sin especificar\": \"No Superhost\",\n",
    "    # \"f\": \"No Superhost\",\n",
    "    # \"t\": \"Superhost\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convertir host_response_time a dicotomica\n",
    "# Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n",
    "#     \"within an hour\": 1,\n",
    "#     \"within a few hours\": 2,\n",
    "#     \"within a day\": 3,\n",
    "#     \"a few days or more\": 4,\n",
    "#     \"Sin especificar\": 5\n",
    "# })\n",
    "\n",
    "# # Verificar que la conversión fue correcta\n",
    "# print(Naples[\"host_response_time\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_response_rate', 'host_acceptance_rate', 'host_total_listings_count']]\n",
    "Var_Dep = Naples['host_is_superhost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['f', 'f', 'f', ..., 'f', 'f', 'f'], dtype=object)"
      ]
     },
     "execution_count": 1709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[2458    0]\n",
      " [ 851    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.7428226050166213\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# super o no super - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.7428226050166213\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) beds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tbeds\n",
    "•\tAccommodates\n",
    "•\tBedrooms\n",
    "•\tRoom_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1714,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  2.  3.  2.2 2.1 6.  4.  5.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"beds\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beds\n",
      "Con cama    10778\n",
      "Sin cama      249\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"beds\"] = Naples[\"beds\"].replace({\n",
    "    0: \"Sin cama\",\n",
    "    6: \"Con cama\",\n",
    "    5: \"Con cama\",\n",
    "    4: \"Con cama\",\n",
    "    3: \"Con cama\",\n",
    "    2.2: \"Con cama\",\n",
    "    2.1: \"Con cama\",\n",
    "    2: \"Con cama\",  \n",
    "    1: \"Con cama\"\n",
    "\n",
    "    # 0: \"f\",\n",
    "    # 6: \"t\",\n",
    "    # 5: \"t\",\n",
    "    # 4: \"t\",\n",
    "    # 3: \"t\",\n",
    "    # 2.2: \"t\",\n",
    "    # 2.1: \"t\",\n",
    "    # 2: \"t\",\n",
    "    # 1: \"t\",\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"beds\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Private room' 'Entire home/apt' 'Hotel room' 'Shared room']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"room_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "0    7195\n",
      "1    3621\n",
      "3     128\n",
      "2      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\2653028822.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": 0,  # Privado\n",
    "    \"Private room\": 1,      # Privado\n",
    "    \"Shared room\": 2,       # Compartido\n",
    "    \"Hotel room\": 3         # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['accommodates', 'bedrooms', 'room_type']]\n",
    "Var_Dep = Naples['beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1720,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Con cama', 'Con cama', 'Con cama', ..., 'Con cama', 'Con cama',\n",
       "       'Con cama'], dtype=object)"
      ]
     },
     "execution_count": 1721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[3232    0]\n",
      " [  77    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.976730129948625\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cama o sin cama - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Con cama\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.976730129948625\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Con cama\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) host_has_profile_pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\thost_has_profile_pic\n",
    "•\thost_listings_count\n",
    "•\thost_verifications\n",
    "•\tHost_is_superhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t' 'f']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_has_profile_pic\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_has_profile_pic\n",
      "t    9997\n",
      "f    1030\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Convertir host_has_profile_pic a dicotomica\n",
    "# Naples[\"host_has_profile_pic\"] = Naples[\"host_has_profile_pic\"].replace({\n",
    "#     \"t\": \"Con foto\",\n",
    "#     \"f\": \"Sin foto\",\n",
    "# })\n",
    "\n",
    "# # Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_has_profile_pic\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_verifications\n",
      "5    9342\n",
      "3    1193\n",
      "4     468\n",
      "2      11\n",
      "0       7\n",
      "1       6\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\2773881076.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_verifications\"] = Naples[\"host_verifications\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_verifications a dicotomica\n",
    "Naples[\"host_verifications\"] = Naples[\"host_verifications\"].replace({\n",
    "    \"['email', 'phone']\": 5, # Verificado = 1\n",
    "    \"['email', 'phone', 'work_email']\": 4,\n",
    "    \"['phone']\": 3,\n",
    "    \"['email']\": 2,\n",
    "    \"['phone', 'work_email']\": 1,\n",
    "    \"[]\": 0, # No verificado = 0\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_verifications\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "0    7814\n",
      "1    2825\n",
      "2     388\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n",
    "    \"Sin especificar\": \"2\",\n",
    "    \"f\": \"0\",\n",
    "    \"t\": \"1\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1731,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_listings_count', 'host_verifications', 'host_is_superhost']]\n",
    "Var_Dep = Naples['host_has_profile_pic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1732,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['t', 't', 't', ..., 't', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 1733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[   1  274]\n",
      " [   4 3030]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9170702179176755\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9159867029313992\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.998681608437706\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) host_identity_verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\thost_identity_verified\n",
    "•\tHost_is_superhost\n",
    "•\treview_scores_communication\n",
    "•\treview_scores_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f' 'Sin especificar' 't']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_is_superhost\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "0    7814\n",
      "1    2825\n",
      "2     388\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\1183087986.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n",
    "    \"Sin especificar\": 2,\n",
    "    \"f\": 0,\n",
    "    \"t\": 1\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_is_superhost', 'review_scores_communication', 'review_scores_rating']]\n",
    "Var_Dep = Naples['host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1742,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['t', 't', 't', ..., 't', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 1743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[   0  230]\n",
      " [   0 3079]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9304925959504382\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9304925959504382\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) room_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\troom_type\n",
    "•\tproperty_type\n",
    "•\taccommodates\n",
    "•\tprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 71.   76.   68.   58.   60.   42.   93.3 117.6  90.  100.   91.   35.\n",
      "  99.  103.   75.   62.   70.   80.  105.  140.   55.  124.   85.   53.\n",
      "  82.  110.   46.   40.  159.  120.  109.  126.   20.  102.   89.  183.\n",
      "  94.   83.   61.   63.   57.   78.  127.   65.   50.   74.  160.  187.\n",
      " 145.   48.   95.  164.   43.  113.   73.   86.  139.   64.  175.  144.\n",
      " 119.  148.  190.   72.   30.  123.  135.   92.   38.  153.   47.   67.\n",
      "  45.   93.  114.   97.   66.   59.   96.  150.   28.   31.  118.  104.\n",
      " 193.   39.  156.  106.   22.  180.   79.  128.  151.   81.   84.   23.\n",
      " 107.  162.  101.  147.  167.  132.  115.  166.   88.   27.  174.  130.\n",
      "  69.  170.   33.  178.   52.   77.  161.   54.  158.   87.   37.   98.\n",
      " 125.  122.  186.  169.  179.  108.  117.   56.  195.  177.  129.  116.\n",
      "  36.  134.  111.  168.   13.  194.   18.  112.  121.   51.   26.  146.\n",
      "  34.  184.  141.  155.   49.  131.  137.  163.  173.   32.  149.  133.\n",
      "  29.  136.   44.  191.  152.   21.  142.  154.  189.  188.  143.  176.\n",
      " 192.  165.   14.   25.   15.   41.  138.  172.  171.  182.  157.  185.\n",
      " 181.   16.   17. ]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"price\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "Privado       7195\n",
      "Compartido    3832\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": \"Privado\",  # Privado\n",
    "    \"Private room\": \"Compartido\",      # Privado\n",
    "    \"Shared room\": \"Compartido\",       # Compartido\n",
    "    \"Hotel room\": \"Compartido\"        # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property_type\n",
      "3     4095\n",
      "1     1690\n",
      "9     1366\n",
      "4     1093\n",
      "7      921\n",
      "12     451\n",
      "2      311\n",
      "14     142\n",
      "5      110\n",
      "8      102\n",
      "24     102\n",
      "11      99\n",
      "30      66\n",
      "15      58\n",
      "33      57\n",
      "26      54\n",
      "31      42\n",
      "44      39\n",
      "35      28\n",
      "28      23\n",
      "27      20\n",
      "13      17\n",
      "29      17\n",
      "10      14\n",
      "23      13\n",
      "18      11\n",
      "48      10\n",
      "45       9\n",
      "21       9\n",
      "39       7\n",
      "36       6\n",
      "42       4\n",
      "37       4\n",
      "17       4\n",
      "51       3\n",
      "20       3\n",
      "32       3\n",
      "52       3\n",
      "22       2\n",
      "43       2\n",
      "38       2\n",
      "34       2\n",
      "16       2\n",
      "19       2\n",
      "41       1\n",
      "40       1\n",
      "6        1\n",
      "46       1\n",
      "47       1\n",
      "49       1\n",
      "50       1\n",
      "25       1\n",
      "53       1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\3781815957.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"property_type\"] = Naples[\"property_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Crear diccionario con valores numéricos para cada tipo de propiedad\n",
    "Naples[\"property_type\"] = Naples[\"property_type\"].replace({\n",
    "    'Private room in bed and breakfast': 1,\n",
    "    'Private room in condo': 2,\n",
    "    'Entire rental unit': 3,\n",
    "    'Private room in rental unit': 4,\n",
    "    'Entire loft': 5,\n",
    "    'Entire cottage': 6,\n",
    "    'Entire home': 7,\n",
    "    'Room in bed and breakfast': 8,\n",
    "    'Entire condo': 9,\n",
    "    'Shared room in hostel': 10,\n",
    "    'Room in boutique hotel': 11,\n",
    "    'Entire vacation home': 12,\n",
    "    'Entire villa': 13,\n",
    "    'Private room in home': 14,\n",
    "    'Entire serviced apartment': 15,\n",
    "    'Entire townhouse': 16,\n",
    "    'Private room in hostel': 17,\n",
    "    'Private room': 18,\n",
    "    'Entire bed and breakfast': 19,\n",
    "    'Private room in villa': 20,\n",
    "    'Entire guesthouse': 21,\n",
    "    'Room in hostel': 22,\n",
    "    'Boat': 23,\n",
    "    'Tiny home': 24,\n",
    "    'Dome': 25,\n",
    "    'Room in hotel': 26,\n",
    "    'Shared room in bed and breakfast': 27,\n",
    "    'Entire guest suite': 28,\n",
    "    'Room in serviced apartment': 29,\n",
    "    'Private room in vacation home': 30,\n",
    "    'Shared room in rental unit': 31,\n",
    "    'Shared room in condo': 32,\n",
    "    'Private room in serviced apartment': 33,\n",
    "    'Entire place': 34,\n",
    "    'Private room in guest suite': 35,\n",
    "    'Private room in tiny home': 36,\n",
    "    'Private room in townhouse': 37,\n",
    "    'Private room in boat': 38,\n",
    "    'Private room in loft': 39,\n",
    "    'Earthen home': 40,\n",
    "    'Dammuso': 41,\n",
    "    'Shared room in home': 42,\n",
    "    'Houseboat': 43,\n",
    "    'Private room in guesthouse': 44,\n",
    "    'Private room in casa particular': 45,\n",
    "    'Camper/RV': 46,\n",
    "    'Castle': 47,\n",
    "    'Casa particular': 48,\n",
    "    'Entire home/apt': 49,\n",
    "    'Farm stay': 50,\n",
    "    'Private room in farm stay': 51,\n",
    "    'Room in aparthotel': 52,\n",
    "    'Tent': 53\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"property_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['property_type', 'accommodates', 'price']]\n",
    "Var_Dep = Naples['room_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Privado', 'Compartido', 'Compartido', ..., 'Compartido',\n",
       "       'Compartido', 'Privado'], dtype=object)"
      ]
     },
     "execution_count": 1754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 736  408]\n",
      " [ 443 1722]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.6242578456318915\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cama o sin cama - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Compartido\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.7428226050166213\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.6433566433566433\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Compartido\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) has_availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\thas_availability\n",
    "•\tprice\n",
    "•\tavailability_365\n",
    "•\tnumber_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t' 'Sin especificar' 'f']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"has_availability\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_availability\n",
      "Disponible       10994\n",
      "No disponible       33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir\n",
    "Naples[\"has_availability\"] = Naples[\"has_availability\"].replace({\n",
    "    \"t\": \"Disponible\",\n",
    "    \"f\": \"No disponible\",\n",
    "    \"Sin especificar\": \"No disponible\",\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"has_availability\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['price', 'availability_365', 'number_of_reviews']]\n",
    "Var_Dep = Naples['has_availability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Disponible', 'Disponible', 'Disponible', ..., 'Disponible',\n",
       "       'Disponible', 'Disponible'], dtype=object)"
      ]
     },
     "execution_count": 1811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[3300    0]\n",
      " [   9    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9972801450589301\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Disponible\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9972801450589301\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Disponible\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) instant_bookable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tinstant_bookable\n",
    "•\tmaximum_nights\n",
    "•\tminimum_nights\n",
    "•\thost_response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sin especificar' 'within a few hours' 'within an hour'\n",
      " 'a few days or more' 'within a day']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_response_time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_response_time\n",
      "1    7330\n",
      "5    2468\n",
      "2     642\n",
      "3     393\n",
      "4     194\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\2485131506.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_response_time a dicotomica\n",
    "Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n",
    "    \"within an hour\": 1,\n",
    "    \"within a few hours\": 2,\n",
    "    \"within a day\": 3,\n",
    "    \"a few days or more\": 4,\n",
    "    \"Sin especificar\": 5\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_response_time\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['maximum_nights', 'minimum_nights', 'host_response_time']]\n",
    "Var_Dep = Naples['instant_bookable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['t', 't', 't', ..., 't', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 1821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 433  991]\n",
      " [ 315 1570]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1823,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.5788770053475936\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# rapido o lento - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.6053188274403143\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.30407303370786515\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tbedrooms\n",
    "•\troom_type\n",
    "•\tbathrooms\n",
    "•\tbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  3.  1.4 2.  1.3 0. ]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"bedrooms\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms\n",
      "De 0 a 1 recamara     8181\n",
      "De 2 a 3 recamaras    2846\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir bedrooms a dicotomica\n",
    "Naples[\"bedrooms\"] = Naples[\"bedrooms\"].replace({\n",
    "    0: \"De 0 a 1 recamara\",\n",
    "    1: \"De 0 a 1 recamara\",\n",
    "    1.3: \"De 0 a 1 recamara\",\n",
    "    1.4: \"De 0 a 1 recamara\",\n",
    "    2: \"De 2 a 3 recamaras\",\n",
    "    3: \"De 2 a 3 recamaras\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"bedrooms\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "1    7195\n",
      "2    3621\n",
      "4     128\n",
      "3      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\12446977.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": 1,  # Privado\n",
    "    \"Private room\": 2,      # Privado\n",
    "    \"Shared room\": 3,       # Compartido\n",
    "    \"Hotel room\": 4         # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['room_type', 'bathrooms', 'beds']]\n",
    "Var_Dep = Naples['bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['De 0 a 1 recamara', 'De 0 a 1 recamara', 'De 0 a 1 recamara', ...,\n",
       "       'De 0 a 1 recamara', 'De 0 a 1 recamara', 'De 0 a 1 recamara'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[2342   87]\n",
      " [ 616  264]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.7917511832319134\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cuarto o sin cuarto - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"De 0 a 1 recamara\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9821698398307646\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.9641827912721285\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"De 0 a 1 recamara\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) minimum_nights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tminimum_nights\n",
    "•\tprice\n",
    "•\troom_type\n",
    "•\tavailability_365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Private room' 'Entire home/apt' 'Hotel room' 'Shared room']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"room_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_nights\n",
      "Noche     6072\n",
      "Noches    4955\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir minimum_nights a dicotomica\n",
    "Naples[\"minimum_nights\"] = Naples[\"minimum_nights\"].replace({\n",
    "    1: \"Noche\",\n",
    "    1.5: \"Noche\",\n",
    "    2: \"Noches\",\n",
    "    3: \"Noches\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"minimum_nights\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "1    7195\n",
      "2    3621\n",
      "4     128\n",
      "3      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_29284\\12446977.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": 1,  # Privado\n",
    "    \"Private room\": 2,      # Privado\n",
    "    \"Shared room\": 3,       # Compartido\n",
    "    \"Hotel room\": 4         # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1839,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['price', 'room_type', 'availability_365']]\n",
    "Var_Dep = Naples['minimum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1840,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1841,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Noches', 'Noche', 'Noches', ..., 'Noche', 'Noches', 'Noches'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 990  889]\n",
      " [ 395 1035]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1843,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.7148014440433214\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cuarto o sin cuarto - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Noche\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.6119673617407072\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.7237762237762237\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Noches\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
